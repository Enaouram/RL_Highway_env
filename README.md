# Reinforcement Learning Project Report: Highway Environment

**Authors:** Ayman MOUMEN, Francisco GARCIA, Marouane MAAMAR, Samer LAHOUD

---

This repository contains the code and project report for a Reinforcement Learning (RL) project focused on navigating and learning in a simulated highway environment. The project explored various RL algorithms and parameter tuning strategies to optimize the performance of agents in both discrete and continuous action spaces.

## Project Overview

### 1. Highway-v0 with Discrete Actions

- **Understanding the Environment:** Initial exploration of the "Highway" environment, including state representation, reward structure, and available actions.
- **Implementing a Basic Agent:** Utilized Deep Q-Network (DQN) algorithm to train an agent, addressing challenges such as environment disparities and suboptimal learning.

### 2. Implementing an Agent in the Parking Environment
....

### 3. Implementing an Agent in the Parking Environment

- **Environment Setup and MDP:** Developed and trained an agent within a Markov Decision Process (MDP) using the SAC (Soft Actor-Critic) algorithm.
- **Algorithm and Parameters:** Utilized SAC for continuous action spaces, emphasizing sample efficiency and off-policy learning.

## Key Findings and Insights

- **Parameter Tuning Experiments:** Explored the impact of hyperparameters (e.g., learning rate, tau, sampled goals) on agent performance.
- **Results Analysis:** Evaluated agent success rates, rewards, and convergence across different parameter settings.
- **Visualizations:** Included graphs and figures illustrating agent performance and learning trends.

## Experiments and Notebooks

The project includes detailed experimental notebooks covering:
- Default Parameter Training
- Learning Rate Tuning
- Tau Parameter Tuning
- Sampled Goals Tuning

Each notebook explores specific aspects of parameter tuning and performance analysis.



---

This README provides an overview of the project and its components. For detailed code implementation and experiment results, refer to the project repository linked above. Contributions and feedback are welcome!
