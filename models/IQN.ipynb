{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ca9bb9",
   "metadata": {},
   "source": [
    "# IQN (Implicit Quantile Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa21bdd",
   "metadata": {},
   "source": [
    "IQN (Implicit Quantile Network) is an algorithm that combines distributional RL with deep neural networks. It aims to learn the entire distribution of return rather than just the expected return. By estimating the quantile function of the return distribution, IQN provides more information about the uncertainty and risk associated with different actions. This allows for more robust decision-making in uncertain environments and can lead to better performance, especially in tasks with nonstationary and complex reward structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "650e8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, terminated, next_state)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size1, hidden_size2, n_tau_samples, n_actions):\n",
    "        super(IQN, self).__init__()\n",
    "        self.obs_size = obs_size\n",
    "        self.n_tau_samples = n_tau_samples\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, hidden_size2)\n",
    "        self.fc4 = nn.Linear(hidden_size2, n_actions * n_tau_samples)\n",
    "\n",
    "    def forward(self, x, taus):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # Reshape to (batch_size, n_tau_samples, n_actions)\n",
    "        x = x.view(batch_size, self.n_tau_samples, self.n_actions)\n",
    "\n",
    "        # Compute the quantile values\n",
    "        taus = taus.view(batch_size, self.n_tau_samples, 1)\n",
    "        quantiles = torch.bmm(x, taus).squeeze(2)\n",
    "\n",
    "        return quantiles\n",
    "\n",
    "class IQN_Agent:\n",
    "    def __init__(self,\n",
    "                action_space,\n",
    "                observation_space,\n",
    "                n_tau_samples,\n",
    "                gamma,\n",
    "                batch_size,\n",
    "                buffer_capacity,\n",
    "                update_target_every, \n",
    "                epsilon_start, \n",
    "                decrease_epsilon_factor, \n",
    "                epsilon_min,\n",
    "                learning_rate,\n",
    "                ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.n_tau_samples = n_tau_samples\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.update_target_every = update_target_every\n",
    "        \n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.decrease_epsilon_factor = decrease_epsilon_factor # larger -> more exploration\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Return action according to an epsilon-greedy exploration policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon: \n",
    "            return self.action_space.sample()\n",
    "            \n",
    "        return self.get_best_action(state)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        state = torch.tensor(state).float().flatten().unsqueeze(0)  # Flatten the state and add batch dimension\n",
    "        taus = torch.rand(self.n_tau_samples).unsqueeze(0)  # Generate random quantiles\n",
    "        quantiles = self.q_net(state, taus)\n",
    "        q_values = torch.mean(quantiles, dim=1)  # Average over quantiles\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        state_tensor = torch.tensor(state).float().unsqueeze(0)\n",
    "        action_tensor = torch.tensor([[action]], dtype=torch.int64)  # action should be long for gather\n",
    "        reward_tensor = torch.tensor([reward]).float()\n",
    "        terminated_tensor = torch.tensor([terminated], dtype=torch.int64).float()  # make sure terminated is also float for consistency\n",
    "        next_state_tensor = torch.tensor(next_state).float().unsqueeze(0)\n",
    "\n",
    "        self.buffer.push(state_tensor, action_tensor, reward_tensor, terminated_tensor, next_state_tensor)\n",
    "\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return np.inf\n",
    "        \n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, terminated_batch, next_state_batch = tuple([torch.cat(data) for data in zip(*transitions)])\n",
    "\n",
    "        taus = torch.rand(self.batch_size, self.n_tau_samples)  # Generate random quantiles\n",
    "        current_quantiles = self.q_net(state_batch, taus)\n",
    "        with torch.no_grad():\n",
    "            next_quantiles = self.target_net(next_state_batch, taus)\n",
    "            best_next_actions = torch.argmax(torch.mean(next_quantiles, dim=1), dim=1)  # Get best actions from target net\n",
    "\n",
    "            # Compute target quantiles\n",
    "            target_quantiles = reward_batch.unsqueeze(1) + (1 - terminated_batch.unsqueeze(1)) * self.gamma * next_quantiles[np.arange(self.batch_size), best_next_actions]\n",
    "\n",
    "        # Compute Huber loss\n",
    "        elementwise_loss = self.huber_loss(current_quantiles, target_quantiles.unsqueeze(1)).mean(dim=1).mean(dim=1)\n",
    "\n",
    "        loss = elementwise_loss.mean()\n",
    "\n",
    "        # Optimize the model \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if not((self.n_steps+1) % self.update_target_every): \n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            \n",
    "        self.decrease_epsilon()\n",
    "            \n",
    "        self.n_steps += 1\n",
    "        if terminated: \n",
    "            self.n_eps += 1\n",
    "\n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def get_q(self, state):\n",
    "        \"\"\"\n",
    "        Compute Q function for a state\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).unsqueeze(0)\n",
    "        taus = torch.rand(self.n_tau_samples).unsqueeze(0)  # Generate random quantiles\n",
    "        quantiles = self.q_net(state_tensor, taus)\n",
    "        q_values = torch.mean(quantiles, dim=1)  # Average over quantiles\n",
    "        return q_values.squeeze().detach().numpy()\n",
    "\n",
    "    def decrease_epsilon(self):\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_start - self.epsilon_min) * (\n",
    "                        np.exp(-1. * self.n_eps / self.decrease_epsilon_factor ) )\n",
    "\n",
    "    def reset(self):\n",
    "        hidden_size = 128\n",
    "        obs_size = self.observation_space.shape[0] * self.observation_space.shape[1]  # Assuming 5x5 observation space\n",
    "        n_actions = self.action_space.n\n",
    "        self.buffer = ReplayBuffer(self.buffer_capacity)\n",
    "        self.q_net = IQN(obs_size, hidden_size, hidden_size, self.n_tau_samples, n_actions)\n",
    "        self.target_net = IQN(obs_size, hidden_size, hidden_size, self.n_tau_samples, n_actions)\n",
    "        self.huber_loss = nn.SmoothL1Loss()\n",
    "        self.optimizer = optim.Adam(params=self.q_net.parameters(), lr=self.learning_rate)\n",
    "        self.epsilon = self.epsilon_start\n",
    "        self.n_steps = 0\n",
    "        self.n_eps = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb999ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
