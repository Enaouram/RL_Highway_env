{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc8a43b",
   "metadata": {},
   "source": [
    "# C51 (Categorical 51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214fd364",
   "metadata": {},
   "source": [
    "C51 (Categorical 51) is a variant of distributional reinforcement learning that discretizes the return distribution into a fixed number of support atoms. It uses a deep neural network to estimate the probability mass function of the return distribution over these support atoms. By doing so, C51 enables the agent to learn a more accurate representation of the return distribution, which can lead to improved performance and better exploration in reinforcement learning tasks. It allows for capturing complex and multimodal distributions of returns, facilitating robust and efficient learning in challenging environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2aa5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class CategoricalDQN(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions, n_atoms, v_min, v_max):\n",
    "        super(CategoricalDQN, self).__init__()\n",
    "        self.obs_size = obs_size\n",
    "        self.n_actions = n_actions\n",
    "        self.n_atoms = n_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, n_actions * n_atoms)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = x.view(-1, self.n_actions, self.n_atoms)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "class CategoricalDQN_Agent:\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 observation_space,\n",
    "                 hidden_size,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay,\n",
    "                 learning_rate,\n",
    "                 batch_size,\n",
    "                 n_atoms,\n",
    "                 v_min,\n",
    "                 v_max,\n",
    "                 ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_atoms = n_atoms\n",
    "        self.v_min = v_min\n",
    "        self.v_max = v_max\n",
    "        \n",
    "        self.q_net = CategoricalDQN(observation_space.shape[0], hidden_size, action_space.n, n_atoms, v_min, v_max)\n",
    "        self.optimizer = optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.loss_function = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            probabilities = self.q_net(state).squeeze(0)\n",
    "            q_values = torch.sum(probabilities * torch.linspace(self.v_min, self.v_max, self.n_atoms), dim=-1)\n",
    "            return torch.argmax(q_values).item()\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "        action = torch.tensor(action).unsqueeze(0)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32)\n",
    "        done = torch.tensor([done], dtype=torch.float32)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        current_probs = self.q_net(state)\n",
    "        current_q_values = torch.sum(current_probs * torch.linspace(self.v_min, self.v_max, self.n_atoms), dim=-1)\n",
    "        current_log_probs = F.log_softmax(current_probs, dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_probs = self.q_net(next_state)\n",
    "            next_q_values = torch.sum(next_probs * torch.linspace(self.v_min, self.v_max, self.n_atoms), dim=-1)\n",
    "            next_action = torch.argmax(next_q_values)\n",
    "            next_max_probs = next_probs[0, next_action]\n",
    "\n",
    "        target_probs = torch.zeros_like(current_probs)\n",
    "        target_probs[:, action] = 1\n",
    "        target_probs = target_probs * (1 - done) * self.gamma + reward\n",
    "\n",
    "        loss = self.loss_function(current_log_probs, target_probs.detach())\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def get_epsilon(self, current_step):\n",
    "        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * np.exp(-1.0 * current_step / self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3344ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
