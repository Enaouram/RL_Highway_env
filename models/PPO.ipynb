{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6721b307",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a8890",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO) is an optimization algorithm for training reinforcement learning agents. It aims to find an optimal policy by iteratively updating the policy parameters in small steps while ensuring that the updated policy does not deviate significantly from the previous one. PPO achieves this by constraining the update using a clipped surrogate objective function, which prevents large policy updates that could lead to policy divergence. By balancing exploration and exploitation, PPO effectively learns near-optimal policies in a wide range of environments, making it a popular choice for training RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037ca0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Actor network\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc_actor = nn.Linear(hidden_size, n_actions)\n",
    "        # Critic network\n",
    "        self.fc_critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Actor network outputs logits for each action\n",
    "        logits = self.fc_actor(x)\n",
    "        # Critic network outputs a single value for state value estimation\n",
    "        value = self.fc_critic(x)\n",
    "        return logits, value\n",
    "\n",
    "class PPO_Agent:\n",
    "    def __init__(self,\n",
    "                 action_space,\n",
    "                 observation_space,\n",
    "                 hidden_size,\n",
    "                 gamma,\n",
    "                 clip_ratio,\n",
    "                 ppo_epochs,\n",
    "                 batch_size,\n",
    "                 lr_actor,\n",
    "                 lr_critic,\n",
    "                 ):\n",
    "        self.action_space = action_space\n",
    "        self.observation_space = observation_space\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.ppo_epochs = ppo_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.actor_lr = lr_actor\n",
    "        self.critic_lr = lr_critic\n",
    "        \n",
    "        # Initialize the Actor-Critic model\n",
    "        self.actor_critic = ActorCritic(observation_space.shape[0], hidden_size, action_space.n)\n",
    "        # Initialize optimizers for the actor and critic\n",
    "        self.optimizer_actor = optim.Adam(self.actor_critic.fc_actor.parameters(), lr=self.actor_lr)\n",
    "        self.optimizer_critic = optim.Adam(self.actor_critic.fc_critic.parameters(), lr=self.critic_lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Convert state to tensor\n",
    "        state = torch.tensor(state).float().unsqueeze(0)\n",
    "        # Forward pass through the actor network to get action logits\n",
    "        logits, _ = self.actor_critic(state)\n",
    "        # Convert logits to action probabilities\n",
    "        action_probs = F.softmax(logits, dim=1)\n",
    "        # Sample an action from the action probabilities\n",
    "        action = torch.multinomial(action_probs, num_samples=1)\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, states, actions, rewards, dones, next_states):\n",
    "        # Convert data to tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "\n",
    "        # Get old logits and values\n",
    "        old_logits, old_values = self.actor_critic(states)\n",
    "        _, next_values = self.actor_critic(next_states)\n",
    "\n",
    "        # Compute advantages\n",
    "        advantages = rewards + self.gamma * (1 - dones) * next_values - old_values.detach()\n",
    "\n",
    "        # Update the policy for several epochs\n",
    "        for _ in range(self.ppo_epochs):\n",
    "            # Forward pass to get new logits and values\n",
    "            logits, values = self.actor_critic(states)\n",
    "            # Compute action probabilities and log probabilities\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            log_probs = F.log_softmax(logits, dim=1)\n",
    "            action_log_probs = log_probs.gather(1, actions)\n",
    "\n",
    "            # Compute ratios and surrogate losses\n",
    "            ratio = (probs / torch.exp(action_log_probs)).gather(1, actions)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.smooth_l1_loss(values, rewards + self.gamma * (1 - dones) * next_values)\n",
    "\n",
    "            # Update actor parameters\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            self.optimizer_actor.step()\n",
    "\n",
    "            # Update critic parameters\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.optimizer_critic.step()\n",
    "\n",
    "    def get_value(self, state):\n",
    "        # Convert state to tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        # Forward pass through the critic network to get value\n",
    "        _, value = self.actor_critic(state)\n",
    "        return value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae20f886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
